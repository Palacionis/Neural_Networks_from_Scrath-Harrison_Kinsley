{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import nnfs # package by the book's author to generate random data\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "nnfs.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Full network with categorical cross entropy loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Dense layer\n",
    "\n",
    "class Layer_Dense():\n",
    "  \n",
    "  # layer initialization\n",
    "  def __init__(self, n_inputs, n_neurons):\n",
    "      self.weights = 0.01 * np.random.rand(n_inputs, n_neurons)\n",
    "      self.biases = np.zeros((1, n_neurons))\n",
    "      \n",
    "      \n",
    "      \n",
    "  # forward pass\n",
    "  def forward(self, inputs):\n",
    "    # calculate output values from inputs, weights and biases\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    \n",
    "  \n",
    "# ReLU activation\n",
    "\n",
    "class Activation_Relu():\n",
    "   \n",
    "   # forward pass\n",
    "   def forward(self, inputs):\n",
    "     # calculate output values from inputs\n",
    "     self.output = np.maximum(0,inputs)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Softmax activation\n",
    "\n",
    "class Activation_Softmax():\n",
    "  \n",
    "  # forward pass\n",
    "  def forward(self, inputs):\n",
    "    \n",
    "    # get unnormalized probabilities\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "    \n",
    "    # normalize probabilities\n",
    "    probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "    \n",
    "    self.output = probabilities\n",
    "    \n",
    "    \n",
    "\n",
    "# Common loss class\n",
    "\n",
    "class Loss():\n",
    "  \n",
    "  # calculates the data and regularization losses\n",
    "  # given model output and ground truth values\n",
    "  \n",
    "  def calculate(self, output, y):\n",
    "    # calculate sample loss\n",
    "    sample_losses = self.forward(output, y)\n",
    "    \n",
    "    # calculate mean loss\n",
    "    data_loss = np.mean(sample_losses)\n",
    "    \n",
    "    # return loss\n",
    "    return data_loss\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "# Cross-entropy loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "  \n",
    "  # forward pass\n",
    "  def forward(self, y_pred, y_true):\n",
    "    \n",
    "    # number of samples in a batch\n",
    "    samples = len(y_pred)\n",
    "    \n",
    "    # clip data to prevent division by 0\n",
    "    # clip both sides to not drag the mean towards any value\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    \n",
    "    # probabilites for target values\n",
    "    # only if categorical labels\n",
    "    \n",
    "    if len(y_true.shape) == 1:\n",
    "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "      \n",
    "    # mask values - only for one hot encoded labels\n",
    "    elif len(y_true.shape) == 2:\n",
    "      correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "    \n",
    "    \n",
    "    # Losses\n",
    "    negative_log_likelihoods = -np.log(correct_confidences)\n",
    "    return negative_log_likelihoods"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# create ReLU activation which will be used with Dense layer\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# create second dense layer with 3 input features from the previous layer and 3 output values\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "# create softmax activation which will be used with dense layer\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# create a loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dense1.weights.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# perform the first pass of input data to the dense layer\n",
    "dense1.forward(X)\n",
    "print(dense1.output[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.97437393e-05 3.68895635e-05 8.37819971e-05]\n",
      " [1.46999708e-04 9.15808050e-05 1.40212578e-04]\n",
      " [2.07372344e-04 1.30936343e-04 5.65641167e-05]\n",
      " [2.86790368e-04 1.80765084e-04 1.03854705e-04]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# perform pass through the first activation function which takes the inputs from above - the layer before\n",
    "activation1.forward(dense1.output)\n",
    "print(activation1.output[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.97437393e-05 3.68895635e-05 8.37819971e-05]\n",
      " [1.46999708e-04 9.15808050e-05 1.40212578e-04]\n",
      " [2.07372344e-04 1.30936343e-04 5.65641167e-05]\n",
      " [2.86790368e-04 1.80765084e-04 1.03854705e-04]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# perfrom the second pass to the dense layer which input is the output from the activation function\n",
    "dense2.forward(activation1.output)\n",
    "print(dense2.output[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [4.8766827e-07 1.3575345e-06 3.6410142e-07]\n",
      " [9.3824906e-07 2.7276390e-06 8.3170107e-07]\n",
      " [7.6309243e-07 2.5356903e-06 1.0358194e-06]\n",
      " [1.1570428e-06 3.7448992e-06 1.4574538e-06]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# perform second activation function which takes inputs from the second dense layer\n",
    "activation2.forward(dense2.output)\n",
    "print(activation2.output[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333325 0.33333355 0.33333322]\n",
      " [0.33333313 0.33333373 0.3333331 ]\n",
      " [0.3333331  0.3333337  0.33333322]\n",
      " [0.333333   0.33333385 0.3333331 ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# perfrom a forward pass through loss function it takes the output of a second dense layer and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(f'Loss: {loss}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.0986113548278809\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Adding accuracy, which describes how often the largest confidence if the correct class in terms of a fraction\n",
    "predictions = np.argmax(activation2.output, axis = 1)\n",
    "if len(y.shape) == 2:\n",
    "  y = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f'Accuracy : {accuracy}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 0.36\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('Kursams': conda)"
  },
  "interpreter": {
   "hash": "2a893ff8458ba932f92e4e064d7c215f78239c4614ca04d610e727159f096c4f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}